{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f11d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, resnet34, ResNet50_Weights\n",
    "from torch.amp import autocast\n",
    "\n",
    "# =======================================================================================\n",
    "# === CONFIGURATION & SETTINGS ===\n",
    "# =======================================================================================\n",
    "\n",
    "# --- Enable/Disable Models ---\n",
    "USE_FASHIONPEDIA_MODEL = True\n",
    "USE_UPAR_MODEL = True\n",
    "USE_CELEBA_MODEL = True\n",
    "USE_FAIRFACE_MODEL = True\n",
    "\n",
    "# --- Input Image ---\n",
    "IMAGE_PATH = \"ss.jpg\" # Change this to your image file\n",
    "\n",
    "# --- Model File Paths ---\n",
    "FACE_PROTO = \"deploy.prototxt.txt\"\n",
    "FACE_MODEL = \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "PERSON_PROTO = \"deploy.prototxt\"\n",
    "PERSON_MODEL = \"mobilenet_iter_73000.caffemodel\"\n",
    "CELEBA_WEIGHTS = \"best_celeba_model.pth\"\n",
    "FAIRFACE_WEIGHTS = \"best_fairface_model.pth\"\n",
    "FASHIONPEDIA_WEIGHTS = \"best_fashionpedia_model.pth\"\n",
    "UPAR_WEIGHTS = \"best_upar_model.pth\"\n",
    "CELEBA_CSV = \"celeba_cleaned.csv\"\n",
    "\n",
    "# --- Confidence Thresholds ---\n",
    "FACE_DETECTION_CONF = 0.4\n",
    "PERSON_DETECTION_CONF = 0.5\n",
    "FASHIONPEDIA_PRED_CONF = 0.5\n",
    "\n",
    "# --- Labels & Model-Specific Thresholds ---\n",
    "\n",
    "# FairFace\n",
    "fairface_race_labels = ['Black', 'East Asian', 'Indian', 'Latino_Hispanic', 'Middle Eastern', 'Southeast Asian', 'White']\n",
    "fairface_gender_labels = ['Female', 'Male']\n",
    "fairface_age_labels = ['0-2', '10-19', '20-29', '3-9', '30-39', '40-49', '50-59', '60-69', '70+']\n",
    "fairface_thresholds = {\"race\": 0.50, \"gender\": 0.2, \"age\": 0.2}\n",
    "\n",
    "# CelebA\n",
    "celeba_thresholds = {\n",
    "    \"Male\": 0.95, \"Smiling\": 0.50, \"Wearing_Earrings\": 0.50, \"Heavy_Makeup\": 0.75,\n",
    "    \"No_Beard\": 0.95, \"Eyeglasses\": 0.70, \"Young\": 0.70,\n",
    "}\n",
    "\n",
    "# UPAR\n",
    "upar_label_cols = [\n",
    "    'Accessory-Backpack', 'Accessory-Bag', 'Accessory-Glasses-Normal', 'Accessory-Hat',\n",
    "    'Age-Adult', 'Age-Young', 'Gender-Female', 'Hair-Length-Long', 'Hair-Length-Short'\n",
    "]\n",
    "upar_thresholds = {\n",
    "    'Accessory-Backpack': 0.80, 'Accessory-Bag': 0.75, 'Accessory-Glasses-Normal': 0.85,\n",
    "    'Accessory-Hat': 0.85, 'Age-Adult': 0.85, 'Age-Young': 0.85, 'Gender-Female': 0.85,\n",
    "    'Hair-Length-Long': 0.85, 'Hair-Length-Short': 0.85\n",
    "}\n",
    "# Full list for UPAR model's 30-class output mapping\n",
    "upar_full_label_list = [\n",
    "    'Accessory-Backpack', 'Accessory-Bag', 'Accessory-Glasses-Normal', 'Accessory-Hat', 'Age-Adult',\n",
    "    'Age-Young', 'Gender-Female', 'Hair-Length-Long', 'Hair-Length-Short', 'LowerBody-Color-Black',\n",
    "    'LowerBody-Color-Blue', 'LowerBody-Color-Brown', 'LowerBody-Color-Grey', 'LowerBody-Color-Other',\n",
    "    'LowerBody-Color-White', 'LowerBody-Length-Short', 'LowerBody-Type-Skirt&Dress',\n",
    "    'LowerBody-Type-Trousers&Shorts', 'UpperBody-Color-Black', 'UpperBody-Color-Blue', 'UpperBody-Color-Brown',\n",
    "    'UpperBody-Color-Green', 'UpperBody-Color-Grey', 'UpperBody-Color-Other', 'UpperBody-Color-Pink',\n",
    "    'UpperBody-Color-Purple', 'UpperBody-Color-Red', 'UpperBody-Color-White', 'UpperBody-Color-Yellow',\n",
    "    'UpperBody-Length-Short'\n",
    "]\n",
    "\n",
    "\n",
    "# Fashionpedia\n",
    "fashionpedia_attributes = {\n",
    "    218: \"Has patch pockets\", 204: \"Regular sleeves\", 205: \"Dropped shoulders\", 159: \"3/4 sleeves\",\n",
    "    163: \"Shirt-style collar\", 225: \"One row of buttons\", 295: \"No extra material\", 137: \"Loose fit\",\n",
    "    145: \"No defined waist\", 115: \"Even on both sides\", 148: \"Very short\", 149: \"Mini length\",\n",
    "    316: \"Nothing special in build\", 317: \"Plain design\", 160: \"Sleeves to the wrist\", 128: \"Straight cut\",\n",
    "    135: \"Tight fit\", 106: \"Form-fitting\", 140: \"Waist sits low\", 302: \"Gathered fabric\", 151: \"Knee length\",\n",
    "    162: \"Standard collar\", 224: \"Pockets with flaps\", 214: \"Puffy long sleeves\", 133: \"Short in front, long in back\", 103: \"Shirt-dress style\",\n",
    "    127: \"Narrow shape\", 325: \"Floral print\", 102: \"Simple straight dress\", 301: \"Printed pattern\", 142: \"Normal waistline\",\n",
    "    200: \"Straight neckline\", 179: \"No collar\", 36: \"Denim pants\", 230: \"Zipper fly\", 136: \"Standard fit\",\n",
    "    298: \"Washed look\", 154: \"Full leg coverage\", 223: \"Curved pocket shape\", 114: \"Asymmetrical look\", 147: \"Length to the hips\",\n",
    "    112: \"Tunic-style\", 309: \"Has a slit\", 152: \"A little below the knee\", 182: \"Round neck\", 311: \"Lined inside\",\n",
    "    146: \"Ends above the hips\", 229: \"Zipper closure\", 305: \"Wrinkled effect\", 312: \"Has appliqués\", 185: \"Oval neck\",\n",
    "    186: \"U-shaped neck\", 119: \"Fitted top, flared bottom\", 141: \"High waistline\", 319: \"Cartoon graphics\", 300: \"Frayed edges\",\n",
    "    174: \"Notch-style lapel\", 38: \"Leggings\", 17: \"Blazer-style\", 322: \"Checkered print\", 138: \"Baggy fit\", 289: \"Fur material\",\n",
    "    304: \"Pleated fabric\", 155: \"To the floor\", 20: \"Motorcycle jacket\", 153: \"Mid-calf length\", 283: \"Metal details\", 10: \"Camisole\",\n",
    "    187: \"Heart-shaped neckline\", 150: \"Above the knee\", 157: \"Short length\", 318: \"Abstract print\",\n",
    "    108: \"Gown style\", 120: \"Trumpet shape\", 95: \"Halter style\", 328: \"Striped pattern\", 183: \"V neckline\", 207: \"Short cap sleeves\",\n",
    "    129: \"A-line shape\", 143: \"Low waist\", 68: \"Slim skirt\", 213: \"Loose sleeve style\",\n",
    "    192: \"Very deep neckline\", 314: \"Metal rivets\", 219: \"Inset pockets\", 222: \"Slanted pockets\", 126: \"Peg pants\",\n",
    "    194: \"Strap around neck\", 209: \"Puffy short sleeves\", 132: \"Wide-leg pants\", 308: \"Cutout design\", 216: \"Loose kimono sleeves\",\n",
    "    281: \"Plastic material\", 118: \"Flared shape\", 113: \"Short flared dress\", 117: \"Circular shape\", 177: \"Fancy lapel\",\n",
    "    226: \"Two rows of buttons\", 297: \"Worn-in look\", 8: \"Cropped top\", 175: \"Pointy lapel\", 326: \"Geometric print\",\n",
    "    50: \"Short shorts\", 181: \"Round crew neck\", 191: \"Square neck\", 11: \"Tank top\", 190: \"Wide scoop neck\",\n",
    "    323: \"Polka dots\", 220: \"Big front pocket\", 0: \"Basic tee\", 320: \"Letters or numbers\", 158: \"Sleeves to elbow\",\n",
    "    210: \"Bell-shaped sleeves\", 123: \"Bell-shaped bottom\", 315: \"Has sequins\", 197: \"High neck\", 101: \"Simple loose dress\",\n",
    "    189: \"Boat-shaped neckline\", 166: \"Wrapped collar\", 180: \"Uneven neckline\", 198: \"Turtleneck\",\n",
    "    307: \"Layered fabric\", 176: \"Wide collar\", 228: \"Wrap-around\", 2: \"Undershirt\", 286: \"Gem detail\", 313: \"Beaded design\",\n",
    "    202: \"Off-shoulder style\", 121: \"Mermaid shape\", 221: \"Stitched pockets\", 234: \"No visible opening\", 203: \"One shoulder\",\n",
    "    206: \"Diagonal sleeve seam\"\n",
    "}\n",
    "\n",
    "# =======================================================================================\n",
    "# === INITIALIZATION ===\n",
    "# =======================================================================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Image Transforms ---\n",
    "# ✅ CORRECTED: Define a separate transform for each model\n",
    "fairface_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "celeba_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "upar_transform = transforms.Compose([\n",
    "    transforms.Resize((231, 93)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "fashionpedia_transform = transforms.Compose([\n",
    "    transforms.Resize((231, 93)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# =======================================================================================\n",
    "# === MODEL & DETECTOR LOADING ===\n",
    "# =======================================================================================\n",
    "\n",
    "# --- Load Object Detectors ---\n",
    "face_detector_net = cv2.dnn.readNetFromCaffe(FACE_PROTO, FACE_MODEL)\n",
    "if USE_FASHIONPEDIA_MODEL or USE_UPAR_MODEL:\n",
    "    person_detector_net = cv2.dnn.readNetFromCaffe(PERSON_PROTO, PERSON_MODEL)\n",
    "\n",
    "# --- Load CelebA Model ---\n",
    "if USE_CELEBA_MODEL:\n",
    "    celeba_model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    celeba_model.fc = nn.Linear(celeba_model.fc.in_features, 35) # Ensure this matches your trained model\n",
    "    celeba_model.load_state_dict(torch.load(CELEBA_WEIGHTS, map_location=device, weights_only=True))\n",
    "    celeba_model.eval().to(device)\n",
    "    celeba_attrs_list = pd.read_csv(CELEBA_CSV).columns[1:].tolist()\n",
    "\n",
    "# --- Load FairFace Model ---\n",
    "if USE_FAIRFACE_MODEL:\n",
    "    class FairFaceMultiTask(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(FairFaceMultiTask, self).__init__()\n",
    "            base = resnet34(weights=None)\n",
    "            self.backbone = nn.Sequential(*list(base.children())[:-1])\n",
    "            self.fc = nn.Linear(512, 18)\n",
    "        def forward(self, x):\n",
    "            x = self.backbone(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            out = self.fc(x)\n",
    "            return out[:, :7], out[:, 7:9], out[:, 9:]\n",
    "\n",
    "    fairface_model = FairFaceMultiTask().to(device)\n",
    "    fairface_model.load_state_dict(torch.load(FAIRFACE_WEIGHTS, map_location=device, weights_only=True))\n",
    "    fairface_model.eval()\n",
    "\n",
    "# --- Load Fashionpedia Model ---\n",
    "if USE_FASHIONPEDIA_MODEL:\n",
    "    class FashionpediaModel(nn.Module):\n",
    "        def __init__(self, num_attributes):\n",
    "            super(FashionpediaModel, self).__init__()\n",
    "            base = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights=None, verbose=False)\n",
    "\n",
    "            in_features = base.fc.in_features\n",
    "            # CORRECTED: Restore the nn.Sequential and nn.Dropout to match the saved model\n",
    "            base.fc = nn.Sequential(\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(in_features, num_attributes)\n",
    "            )\n",
    "            self.model = base\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "    fashionpedia_model = FashionpediaModel(num_attributes=len(fashionpedia_attributes)).to(device)\n",
    "    fashionpedia_model.load_state_dict(torch.load(FASHIONPEDIA_WEIGHTS, map_location=device, weights_only=True))\n",
    "    fashionpedia_model.eval()\n",
    "\n",
    "# --- Load UPAR Model ---\n",
    "if USE_UPAR_MODEL:\n",
    "    class UPARModel(nn.Module):\n",
    "        def __init__(self, num_classes=30):\n",
    "            super(UPARModel, self).__init__()\n",
    "            base = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights=None, verbose=False)\n",
    "\n",
    "            in_features = base.fc.in_features\n",
    "            # CORRECTED: Restore the nn.Sequential and nn.Dropout to match the saved model\n",
    "            base.fc = nn.Sequential(\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(in_features, num_classes)\n",
    "            )\n",
    "            self.model = base\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "    upar_model = UPARModel(num_classes=30).to(device)\n",
    "    upar_model.load_state_dict(torch.load(UPAR_WEIGHTS, map_location=device, weights_only=True))\n",
    "    upar_model.eval()\n",
    "    # Create the index map for efficient lookup\n",
    "    upar_label_idx_map = {label: upar_full_label_list.index(label) for label in upar_label_cols}\n",
    "\n",
    "\n",
    "# =======================================================================================\n",
    "# === HELPER FUNCTIONS (DETECTION, PREDICTION, ANNOTATION) ===\n",
    "# =======================================================================================\n",
    "\n",
    "def detect_faces(image, confidence_threshold):\n",
    "    h, w = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), [104, 117, 123], False, False)\n",
    "    face_detector_net.setInput(blob)\n",
    "    detections = face_detector_net.forward()\n",
    "    boxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        if detections[0, 0, i, 2] > confidence_threshold:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            boxes.append(box.astype(\"int\"))\n",
    "    return boxes\n",
    "\n",
    "def detect_people(image, confidence_threshold):\n",
    "    h, w = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.007843, (300, 300), 127.5)\n",
    "    person_detector_net.setInput(blob)\n",
    "    detections = person_detector_net.forward()\n",
    "    boxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        conf = detections[0, 0, i, 2]\n",
    "        class_id = int(detections[0, 0, i, 1])\n",
    "        if conf > confidence_threshold and class_id == 15: # Class ID 15 is 'person'\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            boxes.append(box.astype(\"int\"))\n",
    "    return boxes\n",
    "\n",
    "def predict_celeba(face_crop):\n",
    "    h, w = face_crop.shape[:2]\n",
    "    pad_h = int(h * 0.1)\n",
    "    pad_w = int(w * 0.1)\n",
    "\n",
    "    padded_crop = cv2.copyMakeBorder(\n",
    "        face_crop, pad_h, pad_h, pad_w, pad_w, borderType=cv2.BORDER_CONSTANT, value=[0, 0, 0]\n",
    "    )\n",
    "\n",
    "    face_pil = Image.fromarray(cv2.cvtColor(padded_crop, cv2.COLOR_BGR2RGB))\n",
    "    # ✅ CORRECTED: Use the specific transform for CelebA\n",
    "    face_tensor = celeba_transform(face_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad(), autocast(device_type=device.type):\n",
    "        output = celeba_model(face_tensor)\n",
    "        probs = torch.sigmoid(output).cpu().squeeze()\n",
    "\n",
    "    pred_attrs = []\n",
    "    for attr, prob in zip(celeba_attrs_list, probs):\n",
    "        if prob.item() > celeba_thresholds.get(attr, 0.8):  # default 0.8 if not set\n",
    "            pred_attrs.append(attr.replace(\"_\", \" \"))\n",
    "\n",
    "    if \"Male\" in pred_attrs:\n",
    "        pred_attrs = [p for p in pred_attrs if p != \"Male\"]\n",
    "        pred_attrs.insert(0, \"Male\")\n",
    "    else:\n",
    "        pred_attrs.insert(0, \"Female\")\n",
    "    return pred_attrs\n",
    "\n",
    "\n",
    "def predict_fairface(face_crop):\n",
    "    h, w = face_crop.shape[:2]\n",
    "    pad_h = int(h * 0.1)\n",
    "    pad_w = int(w * 0.1)\n",
    "\n",
    "    padded_crop = cv2.copyMakeBorder(\n",
    "        face_crop, pad_h, pad_h, pad_w, pad_w, borderType=cv2.BORDER_CONSTANT, value=[0, 0, 0]\n",
    "    )\n",
    "\n",
    "    face_pil = Image.fromarray(cv2.cvtColor(padded_crop, cv2.COLOR_BGR2RGB))\n",
    "    # ✅ CORRECTED: Use the specific transform for FairFace\n",
    "    face_tensor = fairface_transform(face_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad(), autocast(device_type=device.type):\n",
    "        out_race, out_gender, out_age = fairface_model(face_tensor)\n",
    "        race_probs, gender_probs, age_probs = [p.softmax(1).cpu().squeeze() for p in [out_race, out_gender, out_age]]\n",
    "\n",
    "    texts = []\n",
    "    if race_probs.max(0).values.item() > fairface_thresholds[\"race\"]:\n",
    "        texts.append(fairface_race_labels[race_probs.argmax().item()])\n",
    "    if gender_probs.max(0).values.item() > fairface_thresholds[\"gender\"]:\n",
    "        texts.append(fairface_gender_labels[gender_probs.argmax().item()])\n",
    "    if age_probs.max(0).values.item() > fairface_thresholds[\"age\"]:\n",
    "        texts.append(f\"Age: {fairface_age_labels[age_probs.argmax().item()]}\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "def predict_fashion(person_crop):\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB))\n",
    "    # ✅ CORRECTED: Use the specific transform for Fashionpedia\n",
    "    input_tensor = fashionpedia_transform(image_pil).unsqueeze(0).to(device)\n",
    "    with torch.no_grad(), autocast(device_type=device.type):\n",
    "        output = fashionpedia_model(input_tensor)\n",
    "        probs = torch.sigmoid(output).squeeze()\n",
    "\n",
    "    return [\n",
    "        name for i, name in enumerate(fashionpedia_attributes.values())\n",
    "        if probs[i].item() > FASHIONPEDIA_PRED_CONF\n",
    "    ]\n",
    "\n",
    "def predict_upar(person_crop):\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB))\n",
    "    # ✅ CORRECTED: Use the specific transform for UPAR\n",
    "    input_tensor = upar_transform(image_pil).unsqueeze(0).to(device)\n",
    "    with torch.no_grad(), autocast(device_type=device.type):\n",
    "        output = upar_model(input_tensor)\n",
    "        probs = torch.sigmoid(output).squeeze()\n",
    "\n",
    "    predicted = []\n",
    "    for label in upar_label_cols:\n",
    "        idx = upar_label_idx_map[label]\n",
    "        if probs[idx].item() > upar_thresholds[label]:\n",
    "            predicted.append(label)\n",
    "    return predicted\n",
    "\n",
    "def draw_annotations(image, box, texts, position='below', color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Draws compact annotations scaled based on total image size (not object box).\n",
    "    Works better for low-resolution images by using smaller font sizes.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = box\n",
    "    img_h, img_w = image.shape[:2]\n",
    "\n",
    "    # === Smaller Font Scale Based on Total Image Height ===\n",
    "    font_scale = np.clip(img_h / 1600.0, 0.3, 1.2)  # much smaller base\n",
    "    thickness = max(1, int(font_scale * 1.2))\n",
    "\n",
    "    # Get estimated line height\n",
    "    (_, text_h), _ = cv2.getTextSize(\"A\", cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)\n",
    "    padding = int(text_h * 0.25)\n",
    "    line_spacing = int(text_h * 0.35)\n",
    "\n",
    "    # Draw bounding box\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "    # Start label position\n",
    "    y_offset = (y2 + padding) if position == 'below' else (y1)\n",
    "    x_label_start = x1 if position == 'below' else (x2 + padding)\n",
    "\n",
    "    for txt in texts:\n",
    "        (tw, th), _ = cv2.getTextSize(txt, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)\n",
    "        box_top = (x_label_start, y_offset)\n",
    "        box_bot = (x_label_start + tw + padding, y_offset + th + padding)\n",
    "\n",
    "        # Do not draw label if it exceeds image dimensions\n",
    "        if box_bot[0] > img_w or box_bot[1] > img_h:\n",
    "            break\n",
    "\n",
    "        cv2.rectangle(image, box_top, box_bot, color, -1)\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            txt,\n",
    "            (x_label_start + int(padding / 2), y_offset + th),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            font_scale,\n",
    "            (0, 0, 0),\n",
    "            thickness\n",
    "        )\n",
    "        y_offset += th + line_spacing\n",
    "\n",
    "\n",
    "\n",
    "# =======================================================================================\n",
    "# === MAIN EXECUTION ===\n",
    "# =======================================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    original_image = cv2.imread(IMAGE_PATH)\n",
    "    if original_image is None:\n",
    "        raise FileNotFoundError(f\"Error: Could not load image at {IMAGE_PATH}\")\n",
    "\n",
    "    annotated_image = original_image.copy()\n",
    "    H, W = original_image.shape[:2]\n",
    "\n",
    "    # --- 1. Process People for Fashion & UPAR Attributes ---\n",
    "    if USE_FASHIONPEDIA_MODEL or USE_UPAR_MODEL:\n",
    "        print(\"Detecting people for fashion/UPAR analysis...\")\n",
    "        person_boxes = detect_people(annotated_image, PERSON_DETECTION_CONF)\n",
    "        print(f\"Found {len(person_boxes)} people.\")\n",
    "\n",
    "        for p_box in person_boxes:\n",
    "            px, py, px2, py2 = p_box\n",
    "            w, h = px2 - px, py2 - py\n",
    "            \n",
    "            # Use advanced cropping with padding and aspect ratio for better predictions\n",
    "            target_ratio = 231 / 93\n",
    "            padding = int(h * 0.05)\n",
    "            y1 = max(0, py - padding)\n",
    "            new_h = (py2 - y1)\n",
    "            new_w = int(new_h / target_ratio)\n",
    "            cx = px + w // 2\n",
    "            x1 = max(0, cx - new_w // 2)\n",
    "            x2 = min(W, x1 + new_w)\n",
    "            \n",
    "            person_crop = original_image[y1:py2, x1:x2]\n",
    "            if person_crop.size == 0: continue\n",
    "\n",
    "            all_person_preds = []\n",
    "            if USE_UPAR_MODEL:\n",
    "                all_person_preds.extend(predict_upar(person_crop))\n",
    "            if USE_FASHIONPEDIA_MODEL:\n",
    "                all_person_preds.extend(predict_fashion(person_crop))\n",
    "\n",
    "            if all_person_preds:\n",
    "                draw_annotations(annotated_image, (x1, y1, x2, py2), all_person_preds, position='right', color=(0, 255, 0))\n",
    "            else: # Draw box even if no attributes found\n",
    "                 cv2.rectangle(annotated_image, (x1, y1, x2, py2), (0, 255, 0), 2)\n",
    "\n",
    "    # --- 2. Process Faces for Face Attributes ---\n",
    "    if USE_CELEBA_MODEL or USE_FAIRFACE_MODEL:\n",
    "        print(\"Detecting faces for attribute analysis...\")\n",
    "        face_boxes = detect_faces(annotated_image, FACE_DETECTION_CONF)\n",
    "        print(f\"Found {len(face_boxes)} faces.\")\n",
    "\n",
    "        for f_box in face_boxes:\n",
    "            fx1, fy1, fx2, fy2 = f_box\n",
    "            # Center-aligned square crop\n",
    "            fw = fx2 - fx1\n",
    "            fh = fy2 - fy1\n",
    "            cx = fx1 + fw // 2\n",
    "            cy = fy1 + fh // 2\n",
    "\n",
    "            # Apply 40% margin\n",
    "            side = int(max(fw, fh) * 1)\n",
    "\n",
    "            # Shift upward by 10% of side\n",
    "            cy = max(0, cy - int(side * 0.1))\n",
    "\n",
    "            x1 = max(0, cx - side // 2)\n",
    "            y1 = max(0, cy - side // 2)\n",
    "            x2 = min(W, cx + side // 2)\n",
    "            y2 = min(H, cy + side // 2)\n",
    "\n",
    "            face_crop = original_image[y1:y2, x1:x2]\n",
    "            if face_crop.size == 0:\n",
    "                continue\n",
    "\n",
    "            all_face_preds = []\n",
    "            if USE_FAIRFACE_MODEL:\n",
    "                all_face_preds.extend(predict_fairface(face_crop))\n",
    "            if USE_CELEBA_MODEL:\n",
    "                all_face_preds.extend(predict_celeba(face_crop))\n",
    "\n",
    "            if all_face_preds:\n",
    "                draw_annotations(annotated_image, (x1, y1, x2, y2), all_face_preds, position='below', color=(255, 182, 90))  # Use corrected square box\n",
    "            else:\n",
    "                cv2.rectangle(annotated_image, (x1, y1, x2, y2), (255, 182, 90), 2)\n",
    "\n",
    "\n",
    "    # --- 3. Show Final Result ---\n",
    "    plt.figure(figsize=(18, 14))\n",
    "    plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Combined Model Predictions\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, resnet34, ResNet50_Weights\n",
    "from torch.amp import autocast\n",
    "\n",
    "# =======================================================================================\n",
    "# === CONFIGURATION & SETTINGS ===\n",
    "# =======================================================================================\n",
    "\n",
    "# --- Enable/Disable Models ---\n",
    "USE_FASHIONPEDIA_MODEL = True\n",
    "USE_UPAR_MODEL = True\n",
    "USE_CELEBA_MODEL = True\n",
    "USE_FAIRFACE_MODEL = True\n",
    "\n",
    "# --- Input Image ---\n",
    "IMAGE_PATH = \"ss.jpg\" # Change this to your image file\n",
    "\n",
    "# --- Model File Paths ---\n",
    "FACE_PROTO = \"deploy.prototxt.txt\"\n",
    "FACE_MODEL = \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "PERSON_PROTO = \"deploy.prototxt\"\n",
    "PERSON_MODEL = \"mobilenet_iter_73000.caffemodel\"\n",
    "CELEBA_WEIGHTS = \"best_celeba_model.pth\"\n",
    "FAIRFACE_WEIGHTS = \"best_fairface_model.pth\"\n",
    "FASHIONPEDIA_WEIGHTS = \"best_fashionpedia_model.pth\"\n",
    "UPAR_WEIGHTS = \"best_upar_model.pth\"\n",
    "CELEBA_CSV = \"celeba_cleaned.csv\"\n",
    "\n",
    "# --- Confidence Thresholds ---\n",
    "FACE_DETECTION_CONF = 0.4\n",
    "PERSON_DETECTION_CONF = 0.5\n",
    "FASHIONPEDIA_PRED_CONF = 0.5\n",
    "\n",
    "# --- Labels & Model-Specific Thresholds ---\n",
    "\n",
    "# FairFace\n",
    "fairface_race_labels = ['Black', 'East Asian', 'Indian', 'Latino_Hispanic', 'Middle Eastern', 'Southeast Asian', 'White']\n",
    "fairface_gender_labels = ['Female', 'Male']\n",
    "fairface_age_labels = ['0-2', '10-19', '20-29', '3-9', '30-39', '40-49', '50-59', '60-69', '70+']\n",
    "fairface_thresholds = {\"race\": 0.50, \"gender\": 0.2, \"age\": 0.2}\n",
    "\n",
    "# CelebA\n",
    "celeba_thresholds = {\n",
    "    \"Male\": 0.95, \"Smiling\": 0.50, \"Wearing_Earrings\": 0.50, \"Heavy_Makeup\": 0.75,\n",
    "    \"No_Beard\": 0.95, \"Eyeglasses\": 0.70, \"Young\": 0.70,\n",
    "}\n",
    "\n",
    "# UPAR\n",
    "upar_label_cols = [\n",
    "    'Accessory-Backpack', 'Accessory-Bag', 'Accessory-Glasses-Normal', 'Accessory-Hat',\n",
    "    'Age-Adult', 'Age-Young', 'Gender-Female', 'Hair-Length-Long', 'Hair-Length-Short'\n",
    "]\n",
    "upar_thresholds = {\n",
    "    'Accessory-Backpack': 0.80, 'Accessory-Bag': 0.75, 'Accessory-Glasses-Normal': 0.85,\n",
    "    'Accessory-Hat': 0.85, 'Age-Adult': 0.85, 'Age-Young': 0.85, 'Gender-Female': 0.85,\n",
    "    'Hair-Length-Long': 0.85, 'Hair-Length-Short': 0.85\n",
    "}\n",
    "# Full list for UPAR model's 30-class output mapping\n",
    "upar_full_label_list = [\n",
    "    'Accessory-Backpack', 'Accessory-Bag', 'Accessory-Glasses-Normal', 'Accessory-Hat', 'Age-Adult',\n",
    "    'Age-Young', 'Gender-Female', 'Hair-Length-Long', 'Hair-Length-Short', 'LowerBody-Color-Black',\n",
    "    'LowerBody-Color-Blue', 'LowerBody-Color-Brown', 'LowerBody-Color-Grey', 'LowerBody-Color-Other',\n",
    "    'LowerBody-Color-White', 'LowerBody-Length-Short', 'LowerBody-Type-Skirt&Dress',\n",
    "    'LowerBody-Type-Trousers&Shorts', 'UpperBody-Color-Black', 'UpperBody-Color-Blue', 'UpperBody-Color-Brown',\n",
    "    'UpperBody-Color-Green', 'UpperBody-Color-Grey', 'UpperBody-Color-Other', 'UpperBody-Color-Pink',\n",
    "    'UpperBody-Color-Purple', 'UpperBody-Color-Red', 'UpperBody-Color-White', 'UpperBody-Color-Yellow',\n",
    "    'UpperBody-Length-Short'\n",
    "]\n",
    "\n",
    "\n",
    "# Fashionpedia\n",
    "fashionpedia_attributes = {\n",
    "    218: \"Has patch pockets\", 204: \"Regular sleeves\", 205: \"Dropped shoulders\", 159: \"3/4 sleeves\",\n",
    "    163: \"Shirt-style collar\", 225: \"One row of buttons\", 295: \"No extra material\", 137: \"Loose fit\",\n",
    "    145: \"No defined waist\", 115: \"Even on both sides\", 148: \"Very short\", 149: \"Mini length\",\n",
    "    316: \"Nothing special in build\", 317: \"Plain design\", 160: \"Sleeves to the wrist\", 128: \"Straight cut\",\n",
    "    135: \"Tight fit\", 106: \"Form-fitting\", 140: \"Waist sits low\", 302: \"Gathered fabric\", 151: \"Knee length\",\n",
    "    162: \"Standard collar\", 224: \"Pockets with flaps\", 214: \"Puffy long sleeves\", 133: \"Short in front, long in back\", 103: \"Shirt-dress style\",\n",
    "    127: \"Narrow shape\", 325: \"Floral print\", 102: \"Simple straight dress\", 301: \"Printed pattern\", 142: \"Normal waistline\",\n",
    "    200: \"Straight neckline\", 179: \"No collar\", 36: \"Denim pants\", 230: \"Zipper fly\", 136: \"Standard fit\",\n",
    "    298: \"Washed look\", 154: \"Full leg coverage\", 223: \"Curved pocket shape\", 114: \"Asymmetrical look\", 147: \"Length to the hips\",\n",
    "    112: \"Tunic-style\", 309: \"Has a slit\", 152: \"A little below the knee\", 182: \"Round neck\", 311: \"Lined inside\",\n",
    "    146: \"Ends above the hips\", 229: \"Zipper closure\", 305: \"Wrinkled effect\", 312: \"Has appliqués\", 185: \"Oval neck\",\n",
    "    186: \"U-shaped neck\", 119: \"Fitted top, flared bottom\", 141: \"High waistline\", 319: \"Cartoon graphics\", 300: \"Frayed edges\",\n",
    "    174: \"Notch-style lapel\", 38: \"Leggings\", 17: \"Blazer-style\", 322: \"Checkered print\", 138: \"Baggy fit\", 289: \"Fur material\",\n",
    "    304: \"Pleated fabric\", 155: \"To the floor\", 20: \"Motorcycle jacket\", 153: \"Mid-calf length\", 283: \"Metal details\", 10: \"Camisole\",\n",
    "    187: \"Heart-shaped neckline\", 150: \"Above the knee\", 157: \"Short length\", 318: \"Abstract print\",\n",
    "    108: \"Gown style\", 120: \"Trumpet shape\", 95: \"Halter style\", 328: \"Striped pattern\", 183: \"V neckline\", 207: \"Short cap sleeves\",\n",
    "    129: \"A-line shape\", 143: \"Low waist\", 68: \"Slim skirt\", 213: \"Loose sleeve style\",\n",
    "    192: \"Very deep neckline\", 314: \"Metal rivets\", 219: \"Inset pockets\", 222: \"Slanted pockets\", 126: \"Peg pants\",\n",
    "    194: \"Strap around neck\", 209: \"Puffy short sleeves\", 132: \"Wide-leg pants\", 308: \"Cutout design\", 216: \"Loose kimono sleeves\",\n",
    "    281: \"Plastic material\", 118: \"Flared shape\", 113: \"Short flared dress\", 117: \"Circular shape\", 177: \"Fancy lapel\",\n",
    "    226: \"Two rows of buttons\", 297: \"Worn-in look\", 8: \"Cropped top\", 175: \"Pointy lapel\", 326: \"Geometric print\",\n",
    "    50: \"Short shorts\", 181: \"Round crew neck\", 191: \"Square neck\", 11: \"Tank top\", 190: \"Wide scoop neck\",\n",
    "    323: \"Polka dots\", 220: \"Big front pocket\", 0: \"Basic tee\", 320: \"Letters or numbers\", 158: \"Sleeves to elbow\",\n",
    "    210: \"Bell-shaped sleeves\", 123: \"Bell-shaped bottom\", 315: \"Has sequins\", 197: \"High neck\", 101: \"Simple loose dress\",\n",
    "    189: \"Boat-shaped neckline\", 166: \"Wrapped collar\", 180: \"Uneven neckline\", 198: \"Turtleneck\",\n",
    "    307: \"Layered fabric\", 176: \"Wide collar\", 228: \"Wrap-around\", 2: \"Undershirt\", 286: \"Gem detail\", 313: \"Beaded design\",\n",
    "    202: \"Off-shoulder style\", 121: \"Mermaid shape\", 221: \"Stitched pockets\", 234: \"No visible opening\", 203: \"One shoulder\",\n",
    "    206: \"Diagonal sleeve seam\"\n",
    "}\n",
    "\n",
    "# =======================================================================================\n",
    "# === INITIALIZATION ===\n",
    "# =======================================================================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Image Transforms ---\n",
    "# ✅ CORRECTED: Define a separate transform for each model\n",
    "fairface_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "celeba_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "upar_transform = transforms.Compose([\n",
    "    transforms.Resize((231, 93)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "fashionpedia_transform = transforms.Compose([\n",
    "    transforms.Resize((231, 93)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# =======================================================================================\n",
    "# === MODEL & DETECTOR LOADING ===\n",
    "# =======================================================================================\n",
    "\n",
    "# --- Load Object Detectors ---\n",
    "face_detector_net = cv2.dnn.readNetFromCaffe(FACE_PROTO, FACE_MODEL)\n",
    "if USE_FASHIONPEDIA_MODEL or USE_UPAR_MODEL:\n",
    "    person_detector_net = cv2.dnn.readNetFromCaffe(PERSON_PROTO, PERSON_MODEL)\n",
    "\n",
    "# --- Load CelebA Model ---\n",
    "if USE_CELEBA_MODEL:\n",
    "    celeba_model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    celeba_model.fc = nn.Linear(celeba_model.fc.in_features, 35) # Ensure this matches your trained model\n",
    "    celeba_model.load_state_dict(torch.load(CELEBA_WEIGHTS, map_location=device, weights_only=True))\n",
    "    celeba_model.eval().to(device)\n",
    "    celeba_attrs_list = pd.read_csv(CELEBA_CSV).columns[1:].tolist()\n",
    "\n",
    "# --- Load FairFace Model ---\n",
    "if USE_FAIRFACE_MODEL:\n",
    "    class FairFaceMultiTask(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(FairFaceMultiTask, self).__init__()\n",
    "            base = resnet34(weights=None)\n",
    "            self.backbone = nn.Sequential(*list(base.children())[:-1])\n",
    "            self.fc = nn.Linear(512, 18)\n",
    "        def forward(self, x):\n",
    "            x = self.backbone(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            out = self.fc(x)\n",
    "            return out[:, :7], out[:, 7:9], out[:, 9:]\n",
    "\n",
    "    fairface_model = FairFaceMultiTask().to(device)\n",
    "    fairface_model.load_state_dict(torch.load(FAIRFACE_WEIGHTS, map_location=device, weights_only=True))\n",
    "    fairface_model.eval()\n",
    "\n",
    "# --- Load Fashionpedia Model ---\n",
    "if USE_FASHIONPEDIA_MODEL:\n",
    "    class FashionpediaModel(nn.Module):\n",
    "        def __init__(self, num_attributes):\n",
    "            super(FashionpediaModel, self).__init__()\n",
    "            base = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights=None, verbose=False)\n",
    "\n",
    "            in_features = base.fc.in_features\n",
    "            # CORRECTED: Restore the nn.Sequential and nn.Dropout to match the saved model\n",
    "            base.fc = nn.Sequential(\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(in_features, num_attributes)\n",
    "            )\n",
    "            self.model = base\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "    fashionpedia_model = FashionpediaModel(num_attributes=len(fashionpedia_attributes)).to(device)\n",
    "    fashionpedia_model.load_state_dict(torch.load(FASHIONPEDIA_WEIGHTS, map_location=device, weights_only=True))\n",
    "    fashionpedia_model.eval()\n",
    "\n",
    "# --- Load UPAR Model ---\n",
    "if USE_UPAR_MODEL:\n",
    "    class UPARModel(nn.Module):\n",
    "        def __init__(self, num_classes=30):\n",
    "            super(UPARModel, self).__init__()\n",
    "            base = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights=None, verbose=False)\n",
    "\n",
    "            in_features = base.fc.in_features\n",
    "            # CORRECTED: Restore the nn.Sequential and nn.Dropout to match the saved model\n",
    "            base.fc = nn.Sequential(\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(in_features, num_classes)\n",
    "            )\n",
    "            self.model = base\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "    upar_model = UPARModel(num_classes=30).to(device)\n",
    "    upar_model.load_state_dict(torch.load(UPAR_WEIGHTS, map_location=device, weights_only=True))\n",
    "    upar_model.eval()\n",
    "    # Create the index map for efficient lookup\n",
    "    upar_label_idx_map = {label: upar_full_label_list.index(label) for label in upar_label_cols}\n",
    "\n",
    "\n",
    "# =======================================================================================\n",
    "# === HELPER FUNCTIONS (DETECTION, PREDICTION, ANNOTATION) ===\n",
    "# =======================================================================================\n",
    "\n",
    "def detect_faces(image, confidence_threshold):\n",
    "    h, w = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), [104, 117, 123], False, False)\n",
    "    face_detector_net.setInput(blob)\n",
    "    detections = face_detector_net.forward()\n",
    "    boxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        if detections[0, 0, i, 2] > confidence_threshold:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            boxes.append(box.astype(\"int\"))\n",
    "    return boxes\n",
    "\n",
    "def detect_people(image, confidence_threshold):\n",
    "    h, w = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.007843, (300, 300), 127.5)\n",
    "    person_detector_net.setInput(blob)\n",
    "    detections = person_detector_net.forward()\n",
    "    boxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        conf = detections[0, 0, i, 2]\n",
    "        class_id = int(detections[0, 0, i, 1])\n",
    "        if conf > confidence_threshold and class_id == 15: # Class ID 15 is 'person'\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            boxes.append(box.astype(\"int\"))\n",
    "    return boxes\n",
    "\n",
    "def predict_celeba(face_crop):\n",
    "    h, w = face_crop.shape[:2]\n",
    "    pad_h = int(h * 0.1)\n",
    "    pad_w = int(w * 0.1)\n",
    "\n",
    "    padded_crop = cv2.copyMakeBorder(\n",
    "        face_crop, pad_h, pad_h, pad_w, pad_w, borderType=cv2.BORDER_CONSTANT, value=[0, 0, 0]\n",
    "    )\n",
    "\n",
    "    face_pil = Image.fromarray(cv2.cvtColor(padded_crop, cv2.COLOR_BGR2RGB))\n",
    "    # ✅ CORRECTED: Use the specific transform for CelebA\n",
    "    face_tensor = celeba_transform(face_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad(), autocast(device_type=device.type):\n",
    "        output = celeba_model(face_tensor)\n",
    "        probs = torch.sigmoid(output).cpu().squeeze()\n",
    "\n",
    "    pred_attrs = []\n",
    "    for attr, prob in zip(celeba_attrs_list, probs):\n",
    "        if prob.item() > celeba_thresholds.get(attr, 0.8):  # default 0.8 if not set\n",
    "            pred_attrs.append(attr.replace(\"_\", \" \"))\n",
    "\n",
    "    if \"Male\" in pred_attrs:\n",
    "        pred_attrs = [p for p in pred_attrs if p != \"Male\"]\n",
    "        pred_attrs.insert(0, \"Male\")\n",
    "    else:\n",
    "        pred_attrs.insert(0, \"Female\")\n",
    "    return pred_attrs\n",
    "\n",
    "\n",
    "def predict_fairface(face_crop):\n",
    "    h, w = face_crop.shape[:2]\n",
    "    pad_h = int(h * 0.1)\n",
    "    pad_w = int(w * 0.1)\n",
    "\n",
    "    padded_crop = cv2.copyMakeBorder(\n",
    "        face_crop, pad_h, pad_h, pad_w, pad_w, borderType=cv2.BORDER_CONSTANT, value=[0, 0, 0]\n",
    "    )\n",
    "\n",
    "    face_pil = Image.fromarray(cv2.cvtColor(padded_crop, cv2.COLOR_BGR2RGB))\n",
    "    # ✅ CORRECTED: Use the specific transform for FairFace\n",
    "    face_tensor = fairface_transform(face_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad(), autocast(device_type=device.type):\n",
    "        out_race, out_gender, out_age = fairface_model(face_tensor)\n",
    "        race_probs, gender_probs, age_probs = [p.softmax(1).cpu().squeeze() for p in [out_race, out_gender, out_age]]\n",
    "\n",
    "    texts = []\n",
    "    if race_probs.max(0).values.item() > fairface_thresholds[\"race\"]:\n",
    "        texts.append(fairface_race_labels[race_probs.argmax().item()])\n",
    "    if gender_probs.max(0).values.item() > fairface_thresholds[\"gender\"]:\n",
    "        texts.append(fairface_gender_labels[gender_probs.argmax().item()])\n",
    "    if age_probs.max(0).values.item() > fairface_thresholds[\"age\"]:\n",
    "        texts.append(f\"Age: {fairface_age_labels[age_probs.argmax().item()]}\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "def predict_fashion(person_crop):\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB))\n",
    "    # ✅ CORRECTED: Use the specific transform for Fashionpedia\n",
    "    input_tensor = fashionpedia_transform(image_pil).unsqueeze(0).to(device)\n",
    "    with torch.no_grad(), autocast(device_type=device.type):\n",
    "        output = fashionpedia_model(input_tensor)\n",
    "        probs = torch.sigmoid(output).squeeze()\n",
    "\n",
    "    return [\n",
    "        name for i, name in enumerate(fashionpedia_attributes.values())\n",
    "        if probs[i].item() > FASHIONPEDIA_PRED_CONF\n",
    "    ]\n",
    "\n",
    "def predict_upar(person_crop):\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB))\n",
    "    # ✅ CORRECTED: Use the specific transform for UPAR\n",
    "    input_tensor = upar_transform(image_pil).unsqueeze(0).to(device)\n",
    "    with torch.no_grad(), autocast(device_type=device.type):\n",
    "        output = upar_model(input_tensor)\n",
    "        probs = torch.sigmoid(output).squeeze()\n",
    "\n",
    "    predicted = []\n",
    "    for label in upar_label_cols:\n",
    "        idx = upar_label_idx_map[label]\n",
    "        if probs[idx].item() > upar_thresholds[label]:\n",
    "            predicted.append(label)\n",
    "    return predicted\n",
    "\n",
    "def draw_annotations(image, box, texts, position='below', color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Draws compact annotations scaled based on total image size (not object box).\n",
    "    Works better for low-resolution images by using smaller font sizes.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = box\n",
    "    img_h, img_w = image.shape[:2]\n",
    "\n",
    "    # === Smaller Font Scale Based on Total Image Height ===\n",
    "    font_scale = np.clip(img_h / 1600.0, 0.3, 1.2)  # much smaller base\n",
    "    thickness = max(1, int(font_scale * 1.2))\n",
    "\n",
    "    # Get estimated line height\n",
    "    (_, text_h), _ = cv2.getTextSize(\"A\", cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)\n",
    "    padding = int(text_h * 0.25)\n",
    "    line_spacing = int(text_h * 0.35)\n",
    "\n",
    "    # Draw bounding box\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "    # Start label position\n",
    "    y_offset = (y2 + padding) if position == 'below' else (y1)\n",
    "    x_label_start = x1 if position == 'below' else (x2 + padding)\n",
    "\n",
    "    for txt in texts:\n",
    "        (tw, th), _ = cv2.getTextSize(txt, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)\n",
    "        box_top = (x_label_start, y_offset)\n",
    "        box_bot = (x_label_start + tw + padding, y_offset + th + padding)\n",
    "\n",
    "        # Do not draw label if it exceeds image dimensions\n",
    "        if box_bot[0] > img_w or box_bot[1] > img_h:\n",
    "            break\n",
    "\n",
    "        cv2.rectangle(image, box_top, box_bot, color, -1)\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            txt,\n",
    "            (x_label_start + int(padding / 2), y_offset + th),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            font_scale,\n",
    "            (0, 0, 0),\n",
    "            thickness\n",
    "        )\n",
    "        y_offset += th + line_spacing\n",
    "\n",
    "\n",
    "# =======================================================================================\n",
    "# === MAIN EXECUTION (FOR LIVE CAMERA) ===\n",
    "# =======================================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- 1. Initialize Video Capture ---\n",
    "    cap = cv2.VideoCapture(0) # Use 0 for the default webcam.\n",
    "                            # You might need to change this if you have multiple cameras.\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "    print(\"Starting live detection... Press 'q' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        # --- 2. Read a Frame from the Camera ---\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        # Make a copy of the frame to draw annotations on\n",
    "        annotated_frame = frame.copy()\n",
    "        H, W = frame.shape[:2]\n",
    "\n",
    "        # --- 3. Process People for Fashion & UPAR Attributes ---\n",
    "        # This block is the same as your original script, but uses 'frame' and 'annotated_frame'\n",
    "        if USE_FASHIONPEDIA_MODEL or USE_UPAR_MODEL:\n",
    "            person_boxes = detect_people(frame, PERSON_DETECTION_CONF)\n",
    "            for p_box in person_boxes:\n",
    "                px, py, px2, py2 = p_box\n",
    "                w, h = px2 - px, py2 - py\n",
    "\n",
    "                target_ratio = 231 / 93\n",
    "                padding = int(h * 0.05)\n",
    "                y1 = max(0, py - padding)\n",
    "                new_h = (py2 - y1)\n",
    "                new_w = int(new_h / target_ratio)\n",
    "                cx = px + w // 2\n",
    "                x1 = max(0, cx - new_w // 2)\n",
    "                x2 = min(W, x1 + new_w)\n",
    "\n",
    "                # Use the original 'frame' for prediction to avoid using an already annotated image\n",
    "                person_crop = frame[y1:py2, x1:x2]\n",
    "                if person_crop.size == 0: continue\n",
    "\n",
    "                all_person_preds = []\n",
    "                if USE_UPAR_MODEL:\n",
    "                    all_person_preds.extend(predict_upar(person_crop))\n",
    "                if USE_FASHIONPEDIA_MODEL:\n",
    "                    all_person_preds.extend(predict_fashion(person_crop))\n",
    "\n",
    "                if all_person_preds:\n",
    "                    draw_annotations(annotated_frame, (x1, y1, x2, py2), all_person_preds, position='right', color=(0, 255, 0))\n",
    "                else:\n",
    "                    cv2.rectangle(annotated_frame, (x1, y1, x2, py2), (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "        # --- 4. Process Faces for Face Attributes ---\n",
    "        # This block is also the same, using 'frame' and 'annotated_frame'\n",
    "        if USE_CELEBA_MODEL or USE_FAIRFACE_MODEL:\n",
    "            face_boxes = detect_faces(frame, FACE_DETECTION_CONF)\n",
    "            for f_box in face_boxes:\n",
    "                fx1, fy1, fx2, fy2 = f_box\n",
    "                fw, fh = fx2 - fx1, fy2 - fy1\n",
    "                cx, cy = fx1 + fw // 2, fy1 + fh // 2\n",
    "                side = int(max(fw, fh) * 1)\n",
    "                cy = max(0, cy - int(side * 0.1))\n",
    "                x1, y1 = max(0, cx - side // 2), max(0, cy - side // 2)\n",
    "                x2, y2 = min(W, cx + side // 2), min(H, cy + side // 2)\n",
    "\n",
    "                # Use the original 'frame' for prediction\n",
    "                face_crop = frame[y1:y2, x1:x2]\n",
    "                if face_crop.size == 0: continue\n",
    "\n",
    "                all_face_preds = []\n",
    "                if USE_FAIRFACE_MODEL:\n",
    "                    all_face_preds.extend(predict_fairface(face_crop))\n",
    "                if USE_CELEBA_MODEL:\n",
    "                    all_face_preds.extend(predict_celeba(face_crop))\n",
    "\n",
    "                if all_face_preds:\n",
    "                    draw_annotations(annotated_frame, (x1, y1, x2, y2), all_face_preds, position='below', color=(255, 182, 90))\n",
    "                else:\n",
    "                    cv2.rectangle(annotated_frame, (x1, y1, x2, y2), (255, 182, 90), 2)\n",
    "\n",
    "\n",
    "        # --- 5. Show Final Result in a Window ---\n",
    "        # We use cv2.imshow for real-time display instead of matplotlib\n",
    "        cv2.imshow(\"Live AI Detection\", annotated_frame)\n",
    "\n",
    "        # --- 6. Check for 'q' key to exit the loop ---\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # --- 7. Release Resources ---\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
